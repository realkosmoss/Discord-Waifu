{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rvHXHDJM-pj"
   },
   "source": [
    "Download required shit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run them in order real and then go to file explorer -> output.log and u got the ollama api shit real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zVY8PH9wM75t",
    "outputId": "986df8fc-c206-4583-971c-1c7fec1db733"
   },
   "outputs": [],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!sudo mkdir -p --mode=0755 /usr/share/keyrings & curl -fsSL https://pkg.cloudflare.com/cloudflare-main.gpg | sudo tee /usr/share/keyrings/cloudflare-main.gpg >/dev/null & echo 'deb [signed-by=/usr/share/keyrings/cloudflare-main.gpg] https://pkg.cloudflare.com/cloudflared buster main' | sudo tee /etc/apt/sources.list.d/cloudflared.list & sudo apt-get update && sudo apt-get install cloudflared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnACGv93FZ3G"
   },
   "source": [
    "start website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfEsoMTLHEcI",
    "outputId": "b65723e0-dca1-4a75-be3e-c9faf3ead82a"
   },
   "outputs": [],
   "source": [
    "!pip install flask requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-_f9pyKFQ0D"
   },
   "outputs": [],
   "source": [
    "!nohup ollama serve > real.log 2>&1 &\n",
    "\n",
    "import requests\n",
    "from flask import Flask, request, request, jsonify\n",
    "app = Flask(__name__)\n",
    "session = requests.session()\n",
    "\n",
    "def GetAIResponse(messages, message):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    payload = {\n",
    "        \"model\": \"cas/daredevil-8b-abliterated-dpomix.i1:latest\",\n",
    "        \"stream\": False,\n",
    "        \"messages\": messages + [{\"role\": \"user\", \"content\": message}],\n",
    "        \"keep_alive\": \"-1m\",\n",
    "        \"options\": {\n",
    "            \"num_gpu\": 60,\n",
    "            \"num_thread\": 6,\n",
    "            \"low_vram\": False,\n",
    "            \"num_ctx\": 3000, # MEMORY RETARD!\n",
    "            \"num_predict\": -1, # max TOKENS\n",
    "            \"OLLAMA_NOHISTORY\": True\n",
    "        }\n",
    "    }\n",
    "    response = session.post(url, json=payload, headers = {\"Host\": \"localhost:11434\", \"Content-Type\": \"application/json\"})\n",
    "    message = response.json()\n",
    "    return message\n",
    "\n",
    "@app.route('/create', methods=['POST'])\n",
    "def create():\n",
    "    data = request.json # expecting {\"message\", messages -> list -> []}\n",
    "    messages = data.get(\"messages\", [])\n",
    "    message = data.get(\"message\", \"NO MESSAGE GRAH! grah!\")\n",
    "    return GetAIResponse(messages, message)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    !ollama pull cas/daredevil-8b-abliterated-dpomix.i1:latest\n",
    "    !nohup cloudflared tunnel --url http://localhost:8888 > output.log 2>&1 &\n",
    "    app.run(host='localhost', port=8888)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
